# agents/ppo_trainer.py

from stable_baselines3 import PPO
from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize
from stable_baselines3.common.callbacks import BaseCallback, CallbackList
from stable_baselines3.common.monitor import Monitor

import torch.nn as nn
import os

from config.training_configs import PPOTrainingConfig
from environments.env_registry import EnvironmentRegistry
from agents.rl_utils import SaveOnBestTrainingRewardCallback, EarlyStoppingCallback

class PPOTrainer:
    """Gestionnaire d'entra√Ænement PPO avec am√©liorations"""
    
    def __init__(self, env_config, training_config: PPOTrainingConfig):
        self.env_config = env_config
        self.training_config = training_config
        self.model = None
        self.env = None
        
    def setup(self, env_name: str = 'strategic'):
        """Configure l'environnement et le mod√®le"""

        # Cr√©er le r√©pertoire AVANT de cr√©er l'environnement
        os.makedirs(self.training_config.model_save_path, exist_ok=True)
        os.makedirs(self.training_config.tensorboard_log_path, exist_ok=True)
        
        def make_env():
            env = EnvironmentRegistry.create(env_name, self.env_config)
            return Monitor(env, self.training_config.model_save_path)
        
        # Cr√©ation de l'environnement vectoris√©
        self.env = DummyVecEnv([make_env])
        
        # Normalisation avec param√®tres optimis√©s
        if self.env_config.normalize_observations:
            self.env = VecNormalize(
                self.env,
                norm_obs=True,      # Normalise les observations
                norm_reward=True,   # Normalise les rewards
                clip_obs=10.0,      # Clip les observations normalis√©es
                clip_reward=10.0,   # Clip les rewards normalis√©s
                gamma=self.training_config.gamma
            )
        
        # Mod√®le PPO avec MultiInputPolicy
        self.model = PPO(
            "MultiInputPolicy",
            self.env,
            learning_rate=self.training_config.learning_rate,
            n_steps=self.training_config.n_steps,
            batch_size=self.training_config.batch_size,
            n_epochs=self.training_config.n_epochs,
            gamma=self.training_config.gamma,
            gae_lambda=self.training_config.gae_lambda,
            clip_range=self.training_config.clip_range,
            ent_coef=self.training_config.ent_coef,
            vf_coef=self.training_config.vf_coef,
            max_grad_norm=self.training_config.max_grad_norm,
            policy_kwargs=dict(
                net_arch=dict(
                    pi=self.training_config.policy_arch,  # Architecture pour la policy
                    vf=self.training_config.policy_arch   # Architecture pour la value function
                ),
                activation_fn=nn.ReLU,
                # Ajout de normalisation des features
                normalize_images=False
            ),
            tensorboard_log=self.training_config.tensorboard_log_path,
            verbose=1,
            seed=self.env_config.seed
        )
    
    def train(self):
        """Lance l'entra√Ænement avec callbacks am√©lior√©s"""
        if self.model is None:
            self.setup()
        
        # Cr√©ation du r√©pertoire pour les logs
        os.makedirs(self.training_config.model_save_path, exist_ok=True)
        os.makedirs(self.training_config.tensorboard_log_path, exist_ok=True)
        
        # Callbacks
        callback_list = [
            SaveOnBestTrainingRewardCallback(
                check_freq=self.training_config.save_interval, 
                log_dir=self.training_config.model_save_path, 
                verbose=1
            ),
            # D√©commenter l'early stopping si n√©cessaire
            # EarlyStoppingCallback(
            #     check_freq=self.training_config.save_interval * 2, 
            #     patience=5, 
            #     log_dir=self.training_config.model_save_path, 
            #     verbose=1
            # )
        ]
        
        print("üöÄ D√©but de l'entra√Ænement...")
        print(f"   Total timesteps: {self.training_config.total_timesteps}")
        print(f"   Learning rate: {self.training_config.learning_rate}")
        print(f"   N_steps: {self.training_config.n_steps}")
        print(f"   Batch size: {self.training_config.batch_size}")
        
        self.model.learn(
            total_timesteps=self.training_config.total_timesteps,
            tb_log_name="ppo_pdp_training",
            callback=CallbackList(callback_list),
            progress_bar=True  # Barre de progression
        )
        
        # Sauvegarde finale
        final_model_path = os.path.join(self.training_config.model_save_path, "final_model")
        self.model.save(final_model_path)
        print(f"‚úÖ Mod√®le final sauvegard√©: {final_model_path}")
        
        # Sauvegarde de VecNormalize si utilis√©
        if self.env_config.normalize_observations:
            vec_normalize_path = os.path.join(self.training_config.model_save_path, "vec_normalize.pkl")
            self.env.save(vec_normalize_path)
            print(f"‚úÖ VecNormalize sauvegard√©: {vec_normalize_path}")
    
    def load_model(self, model_path: str):
        """Charge un mod√®le d√©j√† entra√Æn√©"""
        self.model = PPO.load(model_path, env=self.env)
        print(f"‚úÖ Mod√®le charg√© depuis: {model_path}")
