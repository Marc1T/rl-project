# ðŸš€ Guide de DÃ©marrage Rapide - Environnement PDP

## ðŸ“¥ Installation

```bash
# Cloner le projet
cd rl-project

# Installer les dÃ©pendances
pip install -r requirements.txt
```

## ðŸ” Ã‰tape 1: Diagnostic de l'Environnement (5 min)

Lancez le script de diagnostic pour vÃ©rifier que tout fonctionne:

```bash
python scripts/test_env_diagnostic.py
```

**RÃ©sultat attendu:**
```
âœ… PASS: FonctionnalitÃ©s de base
âœ… PASS: Ã‰chelle des rewards
âœ… PASS: CohÃ©rence d'Ã©pisode
âœ… PASS: Normalisation

4/4 tests rÃ©ussis
ðŸŽ‰ Tous les tests sont passÃ©s! L'environnement est prÃªt.
```
---

## ðŸ‹ï¸ Ã‰tape 2: EntraÃ®nement Initial (30-60 min)

### EntraÃ®nement Court (Test)

```bash
python scripts/train.py \
    --products 1 \
    --timesteps 50000 \
    --horizon 12 \
    --env_type strategic
```

**Ce qui se passe:**
- L'environnement est crÃ©Ã© et normalisÃ©
- PPO commence l'entraÃ®nement avec les hyperparamÃ¨tres optimisÃ©s
- Les modÃ¨les sont sauvegardÃ©s toutes les 10k timesteps
- Le meilleur modÃ¨le est sauvegardÃ© automatiquement

**Temps estimÃ©:** ~30 minutes sur CPU, ~10 minutes sur GPU

### EntraÃ®nement Complet

```bash
python scripts/train.py \
    --products 1 \
    --timesteps 200000 \
    --horizon 12 \
    --env_type strategic
```

---

## ðŸ“Š Ã‰tape 3: Monitoring (En parallÃ¨le)

Dans un autre terminal, lancez TensorBoard:

```bash
tensorboard --logdir ./logs/tensorboard/
```

Ouvrez votre navigateur: `http://localhost:6006`

**Surveiller les mÃ©triques :**

---

## ðŸ“ˆ Ã‰tape 4: Ã‰valuation (5 min)

Une fois l'entraÃ®nement terminÃ©:

```bash
python scripts/evaluate.py \
    --model ./models/ppo_pdp_strategic_1prod_[DATE]/best_model \
    --episodes 10 \
    --env_type strategic
```

**RÃ©sultat attendu:**

```
ðŸ“Š PERFORMANCE MOYENNE:
   Reward: -2450.3 Â± 180.2
   Stock final: 75.2
   Niveau service: 0.945
```

**InterprÃ©tation:**
- **Reward:** Plus Ã©levÃ© = meilleur
- **Service level > 0.90:** âœ… Bon
- **Stock final 50-150:** âœ… Ã‰quilibrÃ©

---

## ðŸ”„ Ã‰tape 5: Comparaison avec Baselines (10 min)

Comparez votre modÃ¨le RL avec les stratÃ©gies classiques:

```bash
python scripts/compare_strategies.py
```

**RÃ©sultat attendu:**

```
COMPARAISON DES STRATÃ‰GIES
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ StratÃ©gie           â”‚ Reward    â”‚ Service     â”‚ Stock    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Level               â”‚ -3200.5   â”‚ 0.850       â”‚ 180.2    â”‚
â”‚ Chase               â”‚ -2800.3   â”‚ 0.920       â”‚ 45.8     â”‚
â”‚ Fixed Moderate      â”‚ -3500.1   â”‚ 0.780       â”‚ 220.5    â”‚
â”‚ PPO (votre modÃ¨le)  â”‚ -2450.3   â”‚ 0.945       â”‚ 75.2     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ðŸ† Meilleure stratÃ©gie: PPO
```

---

## ðŸ› Troubleshooting

### ProblÃ¨me 1: Le reward ne s'amÃ©liore pas

**SymptÃ´mes:**
- Le reward moyen stagne
- Le reward oscille sans converger

**Solutions:**
1. VÃ©rifier que VecNormalize est bien utilisÃ©
2. Augmenter le nombre de timesteps (100k â†’ 200k)
3. RÃ©duire le learning rate: `learning_rate: float = 1e-4`
4. Essayer l'environnement `base` au lieu de `strategic`

### ProblÃ¨me 2: Les rewards explosent

**SymptÃ´mes:**
- Rewards > 1000 ou < -10000
- Value loss explose

**Solutions:**
1. VÃ©rifier la normalisation dans `normalizers.py`
2. Augmenter `clip_reward` dans VecNormalize
3. RÃ©duire les poids des rewards dans `environment_configs.py`

### ProblÃ¨me 3: Service level toujours faible

**SymptÃ´mes:**
- Service level < 0.80 aprÃ¨s entraÃ®nement

**Solutions:**
1. Augmenter `service_bonus` weight dans la config
2. Augmenter `shortage_cost` dans `base_config.py`
3. VÃ©rifier que les demandes ne sont pas trop Ã©levÃ©es



## ðŸ“ Structure des RÃ©sultats

AprÃ¨s l'entraÃ®nement, vous devriez avoir:

```
rl-project/
â”œâ”€â”€ models/
â”‚   â””â”€â”€ ppo_pdp_strategic_1prod_20241124_143022/
â”‚       â”œâ”€â”€ best_model.zip          # Meilleur modÃ¨le
â”‚       â”œâ”€â”€ final_model.zip         # ModÃ¨le final
â”‚       â”œâ”€â”€ vec_normalize.pkl       # Normalisation
â”‚       â””â”€â”€ monitor.csv             # Logs d'entraÃ®nement
â”œâ”€â”€ logs/
â”‚   â””â”€â”€ tensorboard/
â”‚       â””â”€â”€ ppo_pdp_training_1/     # Logs TensorBoard
â””â”€â”€ evaluation_metrics.json          # MÃ©triques d'Ã©valuation
```

---

## ðŸŽ¯ Objectifs de Performance

### Niveau DÃ©butant (Baseline)
- âœ… Le modÃ¨le termine l'entraÃ®nement sans erreur
- âœ… Service level > 0.80
- âœ… Reward meilleur que "Fixed Moderate" strategy

### Niveau IntermÃ©diaire
- âœ… Service level > 0.90
- âœ… Reward meilleur que "Level" strategy
- âœ… Stock final entre 50-150

### Niveau AvancÃ©
- âœ… Service level > 0.95
- âœ… Reward meilleur que toutes les baselines
- âœ… Stock stable avec faible variance
- âœ… CoÃ»ts de production optimisÃ©s

---

## ðŸ”§ Configurations AvancÃ©es

### Multi-Produits

```bash
python scripts/train.py \
    --products 3 \
    --timesteps 300000 \
    --horizon 12 \
    --env_type strategic
```

### Horizon Plus Long

```bash
python scripts/train.py \
    --products 1 \
    --timesteps 200000 \
    --horizon 24 \
    --env_type strategic
```

### Environnement de Base (Plus Simple)

```bash
python scripts/train.py \
    --products 1 \
    --timesteps 100000 \
    --horizon 12 \
    --env_type base
```

---

## ðŸ“š Ressources SupplÃ©mentaires

- **Stable-Baselines3 Docs:** https://stable-baselines3.readthedocs.io/
- **PPO Paper:** https://arxiv.org/abs/1707.06347
- **RL Debugging:** https://andyljones.com/posts/rl-debugging.html

