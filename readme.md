# ğŸ­ RLPlanif - Plan Directeur de Production Intelligent

<div align="center">

![Python](https://img.shields.io/badge/Python-3.9+-blue.svg)
![Stable-Baselines3](https://img.shields.io/badge/Stable--Baselines3-2.2+-green.svg)
![Gymnasium](https://img.shields.io/badge/Gymnasium-0.29+-orange.svg)
![Streamlit](https://img.shields.io/badge/Streamlit-1.28+-red.svg)
![License](https://img.shields.io/badge/License-MIT-purple.svg)

**Optimisation du Plan Directeur de Production avec l'Apprentissage par Renforcement**

[Documentation](https://rl-project.readthedocs.io/) Â· [DÃ©mo](#-interface-streamlit) Â· [Installation](#-installation)

</div>

---

## ğŸ‘¨â€ğŸ“ Informations du Projet

| | |
|---|---|
| **Auteur** | NANKOULI Marc Thierry |
| **Encadrant** | Prof. TAWFIK Masrour |
| **Institution** | ENSAM MeknÃ¨s |
| **FiliÃ¨re** | IATD-SI (Intelligence Artificielle et Technologie des DonnÃ©es : SystÃ¨mes Industriels ) |
| **Module** | Reinforcement Learning |
| **AnnÃ©e** | 2025/2026 |

---

## ğŸ¯ PrÃ©sentation

**RLPlanif** est un systÃ¨me d'aide Ã  la dÃ©cision pour l'optimisation du **Plan Directeur de Production (PDP)** utilisant l'algorithme **PPO (Proximal Policy Optimization)**.

### ProblÃ©matique

Comment planifier efficacement la production pour :
- âœ… Satisfaire la demande client
- âœ… Minimiser les coÃ»ts (production, stockage, rupture)
- âœ… Optimiser l'utilisation des capacitÃ©s

### Solution

Un agent RL apprend automatiquement Ã  Ã©quilibrer trois leviers de production :

| Levier | Description | CoÃ»t Relatif |
|--------|-------------|--------------|
| âš™ï¸ **Production rÃ©guliÃ¨re** | CapacitÃ© standard | 1x |
| â° **Heures supplÃ©mentaires** | FlexibilitÃ© additionnelle | 1.5x |
| ğŸ¢ **Sous-traitance** | CapacitÃ© externe | 2x |

---

## âœ¨ FonctionnalitÃ©s

- ğŸ¤– **Agent PPO** entraÃ®nÃ© avec Stable-Baselines3
- ğŸ“Š **Comparaison** avec 4 stratÃ©gies baseline (Lot-for-Lot, Chase, Level, EOQ)
- ğŸ® **Interface Streamlit** interactive pour configuration et visualisation
- ğŸ“ˆ **Visualisations** Plotly des performances
- ğŸ“‹ **Tableaux PDP** dÃ©taillÃ©s exportables
- ğŸ”¬ **Exemples industriels** : Rouleurs, Compresseurs, Usinage

---

## ğŸ“¥ Installation

### PrÃ©requis

- Python 3.9+ (recommandÃ© : 3.11)
- Conda ou pip

### Installation Rapide

```bash
# Cloner le projet
git clone https://github.com/Marc1T/rl-project.git
cd rl-project

# CrÃ©er l'environnement conda
conda create -n rl-sb python=3.11 -y
conda activate rl-sb

# Installer les dÃ©pendances
pip install -r requirements.txt
```

### VÃ©rification

```bash
python scripts/test_env_diagnostic.py
```

**RÃ©sultat attendu :**
```
âœ… PASS: FonctionnalitÃ©s de base
âœ… PASS: Ã‰chelle des rewards
âœ… PASS: CohÃ©rence d'Ã©pisode
âœ… PASS: Normalisation

4/4 tests rÃ©ussis
ğŸ‰ Tous les tests sont passÃ©s!
```

---

## ğŸš€ DÃ©marrage Rapide

### Option 1 : Interface Streamlit (RecommandÃ©)

```bash
streamlit run app.py
```

L'interface s'ouvre sur `http://localhost:8501` avec :
- âš™ï¸ Configuration de l'environnement
- ğŸ‹ï¸ EntraÃ®nement PPO
- ğŸ“Š Ã‰valuation et comparaison
- ğŸ“ˆ Visualisations interactives

### Option 2 : Ligne de Commande

```bash
# EntraÃ®nement
python scripts/train.py --products 1 --timesteps 100000 --horizon 12

# Ã‰valuation
python scripts/evaluate.py --model ./models/[VOTRE_MODELE]/best_model

# Comparaison avec baselines
python scripts/compare_strategies.py
```

---

## ğŸ–¥ï¸ Interface Streamlit

<table>
<tr>
<td width="50%">

### ğŸ  Accueil
- PrÃ©sentation du projet
- Statistiques du systÃ¨me
- Guide de dÃ©marrage

</td>
<td width="50%">

### âš™ï¸ Configuration
- Exemples prÃ©-configurÃ©s
- Configuration personnalisÃ©e
- Import/Export JSON

</td>
</tr>
<tr>
<td>

### ğŸ‹ï¸ EntraÃ®nement
- ParamÃ¨tres PPO ajustables
- Barre de progression
- Logs en temps rÃ©el

</td>
<td>

### ğŸ“Š Ã‰valuation
- Comparaison PPO vs Baselines
- MÃ©triques dÃ©taillÃ©es
- Graphiques interactifs

</td>
</tr>
</table>

---

## ğŸ“ Structure du Projet

```
rl-project/
â”œâ”€â”€ ğŸ–¥ï¸ app.py                 # Interface Streamlit
â”œâ”€â”€ ğŸ“‹ requirements.txt       # DÃ©pendances
â”‚
â”œâ”€â”€ ğŸ® environments/          # Environnements Gymnasium
â”‚   â”œâ”€â”€ base_pdp_env.py
â”‚   â”œâ”€â”€ strategic_pdp_env.py
â”‚   â””â”€â”€ env_registry.py
â”‚
â”œâ”€â”€ ğŸ¤– agents/                # Agents RL
â”‚   â”œâ”€â”€ ppo_trainer.py
â”‚   â”œâ”€â”€ baseline_strategies.py
â”‚   â””â”€â”€ rl_utils.py
â”‚
â”œâ”€â”€ ğŸ§© components/            # Composants modulaires
â”‚   â”œâ”€â”€ demand_generators.py
â”‚   â”œâ”€â”€ cost_calculators.py
â”‚   â”œâ”€â”€ reward_calculators.py
â”‚   â””â”€â”€ ...
â”‚
â”œâ”€â”€ âš™ï¸ config/                # Configurations
â”‚   â”œâ”€â”€ base_config.py
â”‚   â”œâ”€â”€ environment_configs.py
â”‚   â””â”€â”€ real_examples_configs.py
â”‚
â”œâ”€â”€ ğŸ“œ scripts/               # Scripts CLI
â”‚   â”œâ”€â”€ train.py
â”‚   â”œâ”€â”€ evaluate.py
â”‚   â””â”€â”€ compare_strategies.py
â”‚
â”œâ”€â”€ ğŸ“š docs/                  # Documentation MkDocs
â”‚
â”œâ”€â”€ ğŸ“Š models/                # ModÃ¨les sauvegardÃ©s
â””â”€â”€ ğŸ“ˆ logs/                  # Logs TensorBoard
```

---

## ğŸ“Š RÃ©sultats Typiques

### Performance sur l'exemple "Rouleurs"

| StratÃ©gie | CoÃ»t Total | Service Level | Avantage PPO |
|-----------|------------|---------------|--------------|
| **PPO** | **2,450** | **98.5%** | - |
| Lot-for-Lot | 3,200 | 95.2% | +30% |
| Chase | 2,890 | 96.8% | +18% |
| Level | 3,500 | 92.1% | +43% |
| EOQ | 2,750 | 97.3% | +12% |

---

## ğŸ“ˆ Monitoring avec TensorBoard

```bash
tensorboard --logdir logs/tensorboard
```

Ouvrez `http://localhost:6006` pour suivre :
- ğŸ“ˆ RÃ©compense moyenne
- ğŸ“‰ Perte d'entraÃ®nement
- ğŸ² Entropie de la politique

---

## ğŸ”§ Configuration AvancÃ©e

### IntensitÃ© de la Demande

```python
# Dans la configuration
config = PDPConfig(
    demand_intensity='high'  # 'low', 'medium', 'high', 'extreme'
)
```

| IntensitÃ© | Multiplicateur | Effet |
|-----------|----------------|-------|
| `low` | 0.75 | Demande modÃ©rÃ©e |
| `medium` | 0.90 | Standard |
| `high` | 1.05 | Demande Ã©levÃ©e, plus de HS |
| `extreme` | 1.20 | Stress test |

### Multi-Produits

```bash
python scripts/train.py --products 3 --timesteps 300000
```

---

## ğŸ“š Documentation

La documentation complÃ¨te est disponible sur [ReadTheDocs](https://rl-project.readthedocs.io/).

### Sections

- ğŸ“– [Guide de dÃ©marrage](https://rl-project.readthedocs.io/getting-started/installation/)
- ğŸ“ [Concepts thÃ©oriques](https://rl-project.readthedocs.io/concepts/pdp/) (PDP, RL, PPO)
- ğŸ—ï¸ [Architecture](https://rl-project.readthedocs.io/architecture/overview/)
- ğŸ“˜ [API Reference](https://rl-project.readthedocs.io/api/environments/)

---

## ğŸ› ï¸ Technologies

| Technologie | Version | Utilisation |
|-------------|---------|-------------|
| Python | 3.9+ | Langage principal |
| Stable-Baselines3 | 2.2.1 | Algorithme PPO |
| Gymnasium | 0.29.1 | Environnements RL |
| PyTorch | 2.1.0 | Backend deep learning |
| Streamlit | 1.28+ | Interface web |
| Plotly | 5.18+ | Visualisations |
| NumPy/Pandas | Latest | Calcul et donnÃ©es |

---

## ğŸ› ï¸ DÃ©pannage Rapide

### Erreur CUDA
```bash
pip install torch --extra-index-url https://download.pytorch.org/whl/cpu
```

### Erreur de mÃ©moire
RÃ©duire `batch_size` dans la configuration:
```python
training_config = PPOTrainingConfig(
    batch_size=32,  # RÃ©duire de 64 Ã  32
    n_steps=512     # RÃ©duire de 1024
)
```

---

## ğŸ“„ Licence

Ce projet est sous licence MIT. Voir le fichier [LICENSE](LICENSE) pour plus de dÃ©tails.

---

## ğŸ™ Remerciements

- **Prof. TAWFIK Masrour** pour l'encadrement et les conseils
- **ENSAM MeknÃ¨s** pour le cadre acadÃ©mique
- **OpenAI** pour l'algorithme PPO
- **Stable-Baselines3** pour l'implÃ©mentation

---

<div align="center">

**Projet rÃ©alisÃ© par NANKOULI Marc Thierry**  
ENSAM MeknÃ¨s - IATD-SI - 2025/2026

[â¬† Retour en haut](#-rlplanif---plan-directeur-de-production-intelligent)

</div>
