# üöÄ Guide de D√©marrage Rapide - Environnement PDP Corrig√©

## üì• Installation

```bash
# Cloner le projet
cd rl-project

# Installer les d√©pendances
pip install -r requirements.txt
```

## üîç √âtape 1: Diagnostic de l'Environnement (5 min)

Lancez le script de diagnostic pour v√©rifier que tout fonctionne:

```bash
python scripts/test_env_diagnostic.py
```

**R√©sultat attendu:**
```
‚úÖ PASS: Fonctionnalit√©s de base
‚úÖ PASS: √âchelle des rewards
‚úÖ PASS: Coh√©rence d'√©pisode
‚úÖ PASS: Normalisation

4/4 tests r√©ussis
üéâ Tous les tests sont pass√©s! L'environnement est pr√™t.
```

**Si un test √©choue:**
- V√©rifiez que vous avez bien remplac√© les fichiers corrig√©s
- Consultez les messages d'erreur d√©taill√©s

---

## üèãÔ∏è √âtape 2: Entra√Ænement Initial (30-60 min)

### Entra√Ænement Court (Test)

```bash
python scripts/train.py \
    --products 1 \
    --timesteps 50000 \
    --horizon 12 \
    --env_type strategic
```

**Ce qui se passe:**
- L'environnement est cr√©√© et normalis√©
- PPO commence l'entra√Ænement avec les hyperparam√®tres optimis√©s
- Les mod√®les sont sauvegard√©s toutes les 10k timesteps
- Le meilleur mod√®le est sauvegard√© automatiquement

**Temps estim√©:** ~30 minutes sur CPU, ~10 minutes sur GPU

### Entra√Ænement Complet

```bash
python scripts/train.py \
    --products 1 \
    --timesteps 200000 \
    --horizon 12 \
    --env_type strategic
```

---

## üìä √âtape 3: Monitoring (En parall√®le)

Dans un autre terminal, lancez TensorBoard:

```bash
tensorboard --logdir ./logs/tensorboard/
```

Ouvrez votre navigateur: `http://localhost:6006`

**M√©triques √† surveiller:**

1. **ep_rew_mean** (Reward moyen par √©pisode)
   - ‚ùå Mauvais: Reste constant ou diminue
   - ‚úÖ Bon: Augmente progressivement

2. **ep_len_mean** (Longueur moyenne des √©pisodes)
   - Devrait √™tre constant = horizon (12)

3. **value_loss** (Perte de la value function)
   - ‚ùå Mauvais: Explose ou reste tr√®s √©lev√©
   - ‚úÖ Bon: Diminue progressivement

4. **policy_loss** (Perte de la policy)
   - Devrait rester stable et faible

---

## üìà √âtape 4: √âvaluation (5 min)

Une fois l'entra√Ænement termin√©:

```bash
python scripts/evaluate.py \
    --model ./models/ppo_pdp_strategic_1prod_[DATE]/best_model \
    --episodes 10 \
    --env_type strategic
```

**R√©sultat attendu:**

```
üìä PERFORMANCE MOYENNE:
   Reward: -2450.3 ¬± 180.2
   Stock final: 75.2
   Niveau service: 0.945
```

**Interpr√©tation:**
- **Reward:** Plus √©lev√© = meilleur (moins n√©gatif)
- **Service level > 0.90:** ‚úÖ Bon
- **Stock final 50-150:** ‚úÖ √âquilibr√©

---

## üîÑ √âtape 5: Comparaison avec Baselines (10 min)

Comparez votre mod√®le RL avec les strat√©gies classiques:

```bash
python scripts/compare_strategies.py
```

**R√©sultat attendu:**

```
COMPARAISON DES STRAT√âGIES
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Strat√©gie           ‚îÇ Reward    ‚îÇ Service     ‚îÇ Stock    ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ Level               ‚îÇ -3200.5   ‚îÇ 0.850       ‚îÇ 180.2    ‚îÇ
‚îÇ Chase               ‚îÇ -2800.3   ‚îÇ 0.920       ‚îÇ 45.8     ‚îÇ
‚îÇ Fixed Moderate      ‚îÇ -3500.1   ‚îÇ 0.780       ‚îÇ 220.5    ‚îÇ
‚îÇ PPO (votre mod√®le)  ‚îÇ -2450.3   ‚îÇ 0.945       ‚îÇ 75.2     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

üèÜ Meilleure strat√©gie: PPO
```

---

## üêõ Troubleshooting

### Probl√®me 1: Le reward ne s'am√©liore pas

**Sympt√¥mes:**
- Le reward moyen stagne
- Le reward oscille sans converger

**Solutions:**
1. V√©rifier que VecNormalize est bien utilis√©
2. Augmenter le nombre de timesteps (100k ‚Üí 200k)
3. R√©duire le learning rate: `learning_rate: float = 1e-4`
4. Essayer l'environnement `base` au lieu de `strategic`

### Probl√®me 2: Les rewards explosent

**Sympt√¥mes:**
- Rewards > 1000 ou < -10000
- Value loss explose

**Solutions:**
1. V√©rifier la normalisation dans `normalizers.py`
2. Augmenter `clip_reward` dans VecNormalize
3. R√©duire les poids des rewards dans `environment_configs.py`

### Probl√®me 3: Service level toujours faible

**Sympt√¥mes:**
- Service level < 0.80 apr√®s entra√Ænement

**Solutions:**
1. Augmenter `service_bonus` weight dans la config
2. Augmenter `shortage_cost` dans `base_config.py`
3. V√©rifier que les demandes ne sont pas trop √©lev√©es

### Probl√®me 4: Stock n√©gatif permanent

**Sympt√¥mes:**
- Stock toujours < 0
- Co√ªt de shortage tr√®s √©lev√©

**Solutions:**
1. Augmenter `initial_stock` dans la config
2. V√©rifier que `allow_backorders=True`
3. Ajuster les capacit√©s de production

---

## üìÅ Structure des R√©sultats

Apr√®s l'entra√Ænement, vous devriez avoir:

```
rl-project/
‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îî‚îÄ‚îÄ ppo_pdp_strategic_1prod_20241124_143022/
‚îÇ       ‚îú‚îÄ‚îÄ best_model.zip          # Meilleur mod√®le
‚îÇ       ‚îú‚îÄ‚îÄ final_model.zip         # Mod√®le final
‚îÇ       ‚îú‚îÄ‚îÄ vec_normalize.pkl       # Normalisation
‚îÇ       ‚îî‚îÄ‚îÄ monitor.csv             # Logs d'entra√Ænement
‚îú‚îÄ‚îÄ logs/
‚îÇ   ‚îî‚îÄ‚îÄ tensorboard/
‚îÇ       ‚îî‚îÄ‚îÄ ppo_pdp_training_1/     # Logs TensorBoard
‚îî‚îÄ‚îÄ evaluation_metrics.json          # M√©triques d'√©valuation
```

---

## üéØ Objectifs de Performance

### Niveau D√©butant (Baseline)
- ‚úÖ Le mod√®le termine l'entra√Ænement sans erreur
- ‚úÖ Service level > 0.80
- ‚úÖ Reward meilleur que "Fixed Moderate" strategy

### Niveau Interm√©diaire
- ‚úÖ Service level > 0.90
- ‚úÖ Reward meilleur que "Level" strategy
- ‚úÖ Stock final entre 50-150

### Niveau Avanc√©
- ‚úÖ Service level > 0.95
- ‚úÖ Reward meilleur que toutes les baselines
- ‚úÖ Stock stable avec faible variance
- ‚úÖ Co√ªts de production optimis√©s

---

## üîß Configurations Avanc√©es

### Multi-Produits

```bash
python scripts/train.py \
    --products 3 \
    --timesteps 300000 \
    --horizon 12 \
    --env_type strategic
```

### Horizon Plus Long

```bash
python scripts/train.py \
    --products 1 \
    --timesteps 200000 \
    --horizon 24 \
    --env_type strategic
```

### Environnement de Base (Plus Simple)

```bash
python scripts/train.py \
    --products 1 \
    --timesteps 100000 \
    --horizon 12 \
    --env_type base
```

---

## üìö Ressources Suppl√©mentaires

- **Stable-Baselines3 Docs:** https://stable-baselines3.readthedocs.io/
- **PPO Paper:** https://arxiv.org/abs/1707.06347
- **RL Debugging:** https://andyljones.com/posts/rl-debugging.html

---

## ‚úÖ Checklist de V√©rification

Avant de signaler un probl√®me:

- [ ] J'ai lanc√© `test_env_diagnostic.py` et tous les tests passent
- [ ] J'ai v√©rifi√© TensorBoard et les m√©triques sont logiques
- [ ] J'ai compar√© avec les baselines
- [ ] J'ai essay√© avec diff√©rents seeds
- [ ] J'ai v√©rifi√© que VecNormalize est bien sauvegard√©/charg√©

---

## üéâ Prochaines √âtapes

Une fois que votre mod√®le fonctionne bien:

1. **Exp√©rimentation:**
   - Tester diff√©rents poids de reward
   - Essayer diff√©rentes architectures de r√©seau
   - Ajouter des contraintes suppl√©mentaires

2. **Validation:**
   - Tester sur des sc√©narios de demande vari√©s
   - Analyser la robustesse aux perturbations
   - Comparer avec des donn√©es r√©elles

3. **D√©ploiement:**
   - Cr√©er une interface de visualisation
   - Int√©grer avec un syst√®me de gestion de production
   - Mettre en place un monitoring en production

