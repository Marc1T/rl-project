# Entra√Ænement

Guide complet pour entra√Æner des mod√®les PPO avec RLPlanif.

## üöÄ Entra√Ænement Rapide

### Via Streamlit

1. Lancez l'application : `streamlit run app.py`
2. Configurez l'environnement dans **‚öôÔ∏è Configuration**
3. Allez dans **üèãÔ∏è Entra√Ænement PPO**
4. D√©finissez les hyperparam√®tres
5. Cliquez sur **üöÄ Lancer l'entra√Ænement**

### Via CLI

```bash
python scripts/train.py --config rouleurs --timesteps 100000
```

## ‚öôÔ∏è Hyperparam√®tres

### Param√®tres Principaux

| Param√®tre | D√©faut | Description | Impact |
|-----------|--------|-------------|--------|
| `total_timesteps` | 50000 | Nombre total de pas | Plus = meilleur mais plus long |
| `learning_rate` | 3e-4 | Taux d'apprentissage | Trop haut = instable, trop bas = lent |
| `n_steps` | 2048 | Pas par rollout | Plus = plus stable |
| `batch_size` | 64 | Taille mini-batch | D√©pend de la m√©moire |
| `n_epochs` | 10 | √âpoques par update | Plus = surapprentissage possible |
| `gamma` | 0.99 | Facteur de discount | Proche de 1 = vision long terme |

### Recommandations par Cas

=== "Probl√®me Simple (1 produit, ‚â§12 p√©riodes)"

    ```python
    params = {
        'total_timesteps': 50000,
        'learning_rate': 3e-4,
        'n_steps': 1024,
        'batch_size': 64,
    }
    ```

=== "Probl√®me Moyen (1-2 produits, 12-24 p√©riodes)"

    ```python
    params = {
        'total_timesteps': 100000,
        'learning_rate': 3e-4,
        'n_steps': 2048,
        'batch_size': 64,
    }
    ```

=== "Probl√®me Complexe (3+ produits, 24+ p√©riodes)"

    ```python
    params = {
        'total_timesteps': 500000,
        'learning_rate': 1e-4,
        'n_steps': 4096,
        'batch_size': 128,
        'n_epochs': 15,
    }
    ```

## üìä Callbacks

### EarlyStopping

Arr√™te l'entra√Ænement si pas d'am√©lioration :

```python
from stable_baselines3.common.callbacks import EvalCallback, StopTrainingOnNoModelImprovement

stop_callback = StopTrainingOnNoModelImprovement(
    max_no_improvement_evals=10,  # √âvaluations sans am√©lioration
    min_evals=20,                 # √âvaluations minimum
    verbose=1
)

eval_callback = EvalCallback(
    eval_env,
    best_model_save_path='./models/best',
    eval_freq=5000,
    callback_after_eval=stop_callback,
    deterministic=True
)
```

### Checkpoint

Sauvegarde p√©riodique du mod√®le :

```python
from stable_baselines3.common.callbacks import CheckpointCallback

checkpoint_callback = CheckpointCallback(
    save_freq=10000,
    save_path='./models/checkpoints',
    name_prefix='ppo_pdp'
)
```

### Custom Callback

```python
from stable_baselines3.common.callbacks import BaseCallback

class MetricsCallback(BaseCallback):
    def __init__(self, verbose=0):
        super().__init__(verbose)
        self.episode_rewards = []
    
    def _on_step(self) -> bool:
        # Logique personnalis√©e
        if self.locals.get('dones', [False])[0]:
            reward = self.locals['rewards'][0]
            self.episode_rewards.append(reward)
        return True
```

## üîÑ Script d'Entra√Ænement Complet

```python
import os
from datetime import datetime
from pathlib import Path

from stable_baselines3 import PPO
from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize
from stable_baselines3.common.callbacks import (
    EvalCallback, 
    CheckpointCallback,
    StopTrainingOnNoModelImprovement
)

from config import get_example_config
from environments import EnvironmentRegistry

def train(
    config_name: str = 'rouleurs',
    total_timesteps: int = 100000,
    learning_rate: float = 3e-4,
    n_steps: int = 2048,
    batch_size: int = 64,
):
    # Charger la configuration
    config = get_example_config(config_name)
    
    # Cr√©er le dossier de sortie
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    output_dir = Path(f'./models/training_{timestamp}')
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # Cr√©er l'environnement
    def make_env():
        return EnvironmentRegistry.create('strategic', config)
    
    train_env = DummyVecEnv([make_env])
    train_env = VecNormalize(train_env, norm_obs=True, norm_reward=False)
    
    eval_env = DummyVecEnv([make_env])
    eval_env = VecNormalize(eval_env, norm_obs=True, norm_reward=False)
    
    # Callbacks
    stop_callback = StopTrainingOnNoModelImprovement(
        max_no_improvement_evals=15,
        min_evals=10,
        verbose=1
    )
    
    eval_callback = EvalCallback(
        eval_env,
        best_model_save_path=str(output_dir / 'best_model'),
        log_path=str(output_dir),
        eval_freq=2500,
        callback_after_eval=stop_callback,
        deterministic=True,
        verbose=1
    )
    
    checkpoint_callback = CheckpointCallback(
        save_freq=10000,
        save_path=str(output_dir / 'checkpoints'),
        name_prefix='ppo'
    )
    
    # Cr√©er le mod√®le
    model = PPO(
        policy='MultiInputPolicy',
        env=train_env,
        learning_rate=learning_rate,
        n_steps=n_steps,
        batch_size=batch_size,
        n_epochs=10,
        gamma=0.99,
        gae_lambda=0.95,
        clip_range=0.2,
        verbose=1,
        tensorboard_log=str(output_dir / 'tensorboard')
    )
    
    # Entra√Æner
    print(f"üöÄ D√©but de l'entra√Ænement...")
    print(f"üìÅ Dossier de sortie: {output_dir}")
    
    model.learn(
        total_timesteps=total_timesteps,
        callback=[eval_callback, checkpoint_callback],
        progress_bar=True
    )
    
    # Sauvegarder
    model.save(str(output_dir / 'final_model'))
    train_env.save(str(output_dir / 'vec_normalize.pkl'))
    
    print(f"‚úÖ Entra√Ænement termin√©!")
    print(f"üìä Mod√®le sauvegard√© dans: {output_dir}")
    
    return model, train_env

if __name__ == '__main__':
    train()
```

## üìà Monitoring avec TensorBoard

### Lancement

```bash
tensorboard --logdir logs/tensorboard
```

### M√©triques √† Surveiller

| M√©trique | Bon signe | Mauvais signe |
|----------|-----------|---------------|
| `ep_rew_mean` | ‚ÜóÔ∏è Croissant | Stagnant ou oscillant |
| `ep_len_mean` | Stable | Tr√®s variable |
| `loss` | ‚ÜòÔ∏è D√©croissant | Croissant |
| `entropy_loss` | D√©cro√Æt lentement | D√©cro√Æt trop vite |
| `approx_kl` | < 0.02 | > 0.05 |
| `clip_fraction` | 0.1 - 0.3 | > 0.5 |

### Interpr√©tation

??? success "Entra√Ænement R√©ussi"
    - R√©compense croissante puis plateau
    - Perte d√©croissante
    - KL divergence faible
    - Entropie d√©cro√Æt progressivement

??? warning "Probl√®mes Courants"
    **R√©compense stagne** :
    
    - Augmenter `n_steps`
    - Augmenter `ent_coef` pour plus d'exploration
    - V√©rifier la fonction de r√©compense
    
    **Instabilit√© (oscillations)** :
    
    - R√©duire `learning_rate`
    - Augmenter `batch_size`
    - R√©duire `clip_range`

## üîß Tuning Automatique

### Grid Search Simple

```python
from itertools import product

param_grid = {
    'learning_rate': [1e-4, 3e-4, 1e-3],
    'n_steps': [1024, 2048],
    'batch_size': [32, 64],
}

best_reward = -float('inf')
best_params = None

for lr, steps, batch in product(*param_grid.values()):
    model, env = train(
        learning_rate=lr,
        n_steps=steps,
        batch_size=batch,
        total_timesteps=20000  # Court pour le tuning
    )
    
    # √âvaluer
    rewards = evaluate(model, env, n_episodes=10)
    mean_reward = np.mean(rewards)
    
    if mean_reward > best_reward:
        best_reward = mean_reward
        best_params = {'lr': lr, 'steps': steps, 'batch': batch}

print(f"Meilleurs param√®tres: {best_params}")
```

## Prochaine √âtape

‚û°Ô∏è [√âvaluation](evaluation.md)
