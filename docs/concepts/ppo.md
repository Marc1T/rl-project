# Algorithme PPO

## ğŸ“– Introduction

**PPO** (Proximal Policy Optimization) est un algorithme d'apprentissage par renforcement dÃ©veloppÃ© par OpenAI en 2017. Il est devenu l'un des algorithmes les plus populaires grÃ¢ce Ã  son Ã©quilibre entre **simplicitÃ©**, **stabilitÃ©** et **performance**.

## ğŸ¯ Pourquoi PPO ?

| CritÃ¨re | PPO | Autres |
|---------|-----|--------|
| **StabilitÃ©** | â­â­â­ | Variable |
| **Performance** | â­â­â­ | â­â­â­ |
| **SimplicitÃ©** | â­â­â­ | â­â­ |
| **HyperparamÃ¨tres** | Peu sensible | TrÃ¨s sensible |

## ğŸ§® Fonctionnement

### Architecture Actor-Critic

PPO utilise deux rÃ©seaux de neurones :

```mermaid
graph TB
    S[Ã‰tat s] --> A[Actor Ï€_Î¸]
    S --> C[Critic V_Ï†]
    A --> Ac[Action a]
    C --> V[Valeur V(s)]
```

### Fonction Objectif

L'objectif de PPO est de maximiser :

$$L^{CLIP}(\theta) = \mathbb{E}_t \left[ \min(r_t(\theta) \hat{A}_t, \text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon) \hat{A}_t) \right]$$

OÃ¹ :

- $r_t(\theta) = \frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)}$ : Ratio des politiques
- $\hat{A}_t$ : Estimateur de l'avantage
- $\epsilon$ : ParamÃ¨tre de clipping (typiquement 0.2)

### Clipping ExpliquÃ©

Le clipping empÃªche des mises Ã  jour trop importantes :

```
Si Avantage > 0 (bonne action) :
    ratio â‰¤ 1 + Îµ â†’ Limite l'augmentation de probabilitÃ©

Si Avantage < 0 (mauvaise action) :
    ratio â‰¥ 1 - Îµ â†’ Limite la diminution de probabilitÃ©
```

### Fonction de Valeur

La perte du Critic :

$$L^{VF}(\phi) = \mathbb{E}_t \left[ (V_\phi(s_t) - V_t^{target})^2 \right]$$

### Bonus d'Entropie

Pour encourager l'exploration :

$$S[\pi_\theta](s_t) = -\sum_a \pi_\theta(a|s_t) \log \pi_\theta(a|s_t)$$

### Perte Totale

$$L^{TOTAL}(\theta, \phi) = L^{CLIP}(\theta) - c_1 L^{VF}(\phi) + c_2 S[\pi_\theta]$$

## âš™ï¸ HyperparamÃ¨tres

### ParamÃ¨tres Principaux

| ParamÃ¨tre | Valeur DÃ©faut | Description |
|-----------|---------------|-------------|
| `learning_rate` | 3e-4 | Taux d'apprentissage |
| `n_steps` | 2048 | Pas par rollout |
| `batch_size` | 64 | Taille des mini-batches |
| `n_epochs` | 10 | Ã‰poques par update |
| `gamma` | 0.99 | Facteur de discount |
| `gae_lambda` | 0.95 | GAE lambda |
| `clip_range` | 0.2 | ParamÃ¨tre Îµ de clipping |
| `ent_coef` | 0.0 | Coefficient d'entropie |
| `vf_coef` | 0.5 | Coefficient critic |

### Recommandations pour le PDP

```python
ppo_params = {
    'learning_rate': 3e-4,      # Standard
    'n_steps': 2048,            # Rollouts suffisants
    'batch_size': 64,           # Mini-batches
    'n_epochs': 10,             # Passes sur les donnÃ©es
    'gamma': 0.99,              # Horizon long
    'gae_lambda': 0.95,         # GAE standard
    'clip_range': 0.2,          # Clipping standard
    'ent_coef': 0.01,           # LÃ©gÃ¨re exploration
    'max_grad_norm': 0.5,       # Gradient clipping
}
```

## ğŸ”„ Algorithme

```
Pour chaque itÃ©ration :
    1. Collecter T timesteps avec la politique actuelle Ï€_Î¸
    2. Calculer les avantages Ã‚ avec GAE
    3. Pour chaque Ã©poque k = 1, ..., K :
        a. Ã‰chantillonner mini-batches
        b. Calculer le ratio r_t(Î¸)
        c. Calculer la perte clippÃ©e L^CLIP
        d. Mettre Ã  jour Î¸ par gradient ascent
    4. Î¸_old â† Î¸
```

## ğŸ“Š GAE (Generalized Advantage Estimation)

L'avantage est estimÃ© par :

$$\hat{A}_t = \sum_{l=0}^{\infty} (\gamma \lambda)^l \delta_{t+l}$$

OÃ¹ $\delta_t = r_t + \gamma V(s_{t+1}) - V(s_t)$ est l'erreur TD.

## ğŸ› ï¸ ImplÃ©mentation dans RLPlanif

### Configuration

```python
from stable_baselines3 import PPO
from stable_baselines3.common.vec_env import VecNormalize, DummyVecEnv

# CrÃ©er l'environnement
env = DummyVecEnv([lambda: EnvironmentRegistry.create('strategic', config)])
env = VecNormalize(env, norm_obs=True, norm_reward=False)

# CrÃ©er le modÃ¨le PPO
model = PPO(
    policy='MultiInputPolicy',  # Pour Dict observations
    env=env,
    learning_rate=3e-4,
    n_steps=2048,
    batch_size=64,
    n_epochs=10,
    gamma=0.99,
    gae_lambda=0.95,
    clip_range=0.2,
    verbose=1,
    tensorboard_log='./logs/tensorboard'
)

# EntraÃ®ner
model.learn(total_timesteps=100000)
```

### Architecture du RÃ©seau

```python
policy_kwargs = {
    'net_arch': dict(
        pi=[128, 128],  # Actor
        vf=[128, 128]   # Critic
    ),
    'activation_fn': torch.nn.ReLU
}
```

## ğŸ“ˆ Monitoring

### MÃ©triques TensorBoard

| MÃ©trique | Signification |
|----------|---------------|
| `rollout/ep_rew_mean` | RÃ©compense moyenne â†—ï¸ |
| `train/loss` | Perte totale â†˜ï¸ |
| `train/policy_gradient_loss` | Perte actor |
| `train/value_loss` | Perte critic |
| `train/entropy_loss` | Entropie â†˜ï¸ lentement |
| `train/clip_fraction` | Fraction clippÃ©e |
| `train/approx_kl` | KL divergence < 0.02 |

## ğŸ”¬ Tuning

### ProblÃ¨mes Courants

??? warning "RÃ©compense stagne"
    - Augmenter `n_steps` pour plus d'exploration
    - Augmenter `ent_coef` (0.01 â†’ 0.05)
    - VÃ©rifier la fonction de rÃ©compense

??? warning "InstabilitÃ© (oscillations)"
    - RÃ©duire `learning_rate`
    - RÃ©duire `clip_range` (0.2 â†’ 0.1)
    - Augmenter `batch_size`

??? warning "KL divergence Ã©levÃ©e"
    - RÃ©duire `learning_rate`
    - RÃ©duire `n_epochs`

## Prochaine Ã‰tape

â¡ï¸ [Architecture du Projet](../architecture/overview.md)
