# Apprentissage par Renforcement

## üìñ Introduction

L'**Apprentissage par Renforcement** (Reinforcement Learning, RL) est un paradigme d'apprentissage automatique o√π un **agent** apprend √† prendre des d√©cisions en interagissant avec un **environnement**.

```mermaid
graph LR
    A[Agent] -->|Action a| B[Environnement]
    B -->|√âtat s', R√©compense r| A
```

## üéØ Concepts Fondamentaux

### Agent et Environnement

| Concept | Description | Dans RLPlanif |
|---------|-------------|---------------|
| **Agent** | Prend les d√©cisions | Algorithme PPO |
| **Environnement** | Syst√®me √† contr√¥ler | Usine de production |
| **√âtat** | Observation du syst√®me | Stocks, demande, p√©riode |
| **Action** | D√©cision de l'agent | Quantit√©s √† produire |
| **R√©compense** | Feedback sur l'action | Co√ªts et service |

### Processus de D√©cision Markovien (MDP)

Un MDP est d√©fini par le tuple $(S, A, P, R, \gamma)$ :

- $S$ : Espace des √©tats
- $A$ : Espace des actions
- $P(s'|s,a)$ : Probabilit√©s de transition
- $R(s,a,s')$ : Fonction de r√©compense
- $\gamma$ : Facteur de discount

### Politique

Une **politique** $\pi$ d√©finit le comportement de l'agent :

$$\pi(a|s) = P(A_t = a | S_t = s)$$

L'objectif est de trouver la politique optimale $\pi^*$ qui maximise le retour cumul√© :

$$\pi^* = \arg\max_\pi \mathbb{E}\left[\sum_{t=0}^{T} \gamma^t R_t\right]$$

## üìä Formulation pour le PDP

### Espace d'√âtats

Dans RLPlanif, l'√©tat contient :

```python
observation = {
    'stocks': [...],           # Niveaux de stock par produit
    'demand_forecast': [...],  # Pr√©visions de demande
    'current_period': ...,     # P√©riode actuelle
    'capacities': [...]        # Capacit√©s disponibles
}
```

### Espace d'Actions

L'action est un vecteur continu $a \in [0,1]^3$ :

$$a = [a_{regular}, a_{overtime}, a_{subcontracting}]$$

Chaque composante repr√©sente le pourcentage de capacit√© √† utiliser.

### Fonction de R√©compense

La r√©compense combine plusieurs objectifs :

$$R = -\alpha_1 \cdot C_{prod} - \alpha_2 \cdot C_{stock} - \alpha_3 \cdot C_{rupture} + \alpha_4 \cdot ServiceLevel$$

## üß† Algorithmes

### Value-Based Methods

Apprennent la **fonction de valeur** $V(s)$ ou $Q(s,a)$ :

- **Q-Learning** : Off-policy, tabulaire
- **DQN** : Deep Q-Network, utilise des r√©seaux de neurones
- **SARSA** : On-policy

### Policy Gradient Methods

Optimisent directement la **politique** $\pi_\theta$ :

$$\nabla_\theta J(\theta) = \mathbb{E}\left[\nabla_\theta \log \pi_\theta(a|s) \cdot A(s,a)\right]$$

- **REINFORCE** : Monte Carlo policy gradient
- **A2C** : Advantage Actor-Critic
- **PPO** : Proximal Policy Optimization ‚≠ê

### Actor-Critic Methods

Combinent les deux approches :

- **Actor** : Politique $\pi_\theta(a|s)$
- **Critic** : Fonction de valeur $V_\phi(s)$

## üìà Avantages du RL pour le PDP

| Avantage | Description |
|----------|-------------|
| **Adaptabilit√©** | S'adapte aux changements de demande |
| **Optimisation globale** | Vision sur tout l'horizon |
| **Gestion d'incertitude** | Apprend des patterns stochastiques |
| **Multi-objectif** | Balance co√ªts et service |

## ‚öñÔ∏è D√©fis

| D√©fi | Solution dans RLPlanif |
|------|------------------------|
| **Sample efficiency** | PPO avec rollouts efficaces |
| **Stabilit√©** | VecNormalize, gradient clipping |
| **Exploration** | Entropie dans la politique |
| **Convergence** | EarlyStopping callback |

## üî¨ √âvaluation

### M√©triques RL

- **Return moyen** : $\bar{G} = \frac{1}{N}\sum_{i=1}^{N} G_i$
- **√âcart-type** : Stabilit√© des performances
- **Courbe d'apprentissage** : Progression dans le temps

### M√©triques M√©tier

- **Co√ªt total** : Objectif principal
- **Service level** : Satisfaction demande
- **Utilisation capacit√©** : Efficacit√© production

## Prochaine √âtape

‚û°Ô∏è [Algorithme PPO](ppo.md)
