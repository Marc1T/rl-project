# Premier EntraÃ®nement

Ce guide vous accompagne pour votre premier entraÃ®nement avec RLPlanif.

## ğŸ¯ Objectif

EntraÃ®ner un agent PPO sur l'exemple "Rouleurs" et comparer avec les stratÃ©gies baseline.

## Option 1 : Interface Streamlit (RecommandÃ©)

### 1. Lancer l'application

```bash
streamlit run app.py
```

L'interface s'ouvre dans votre navigateur Ã  l'adresse `http://localhost:8501`.

### 2. Configurer l'environnement

1. Cliquez sur **âš™ï¸ Configuration** dans la sidebar
2. SÃ©lectionnez **ğŸ¯ Exemple PrÃ©-configurÃ©**
3. Choisissez **ğŸ”§ Rouleurs (12 pÃ©riodes)**
4. Cliquez sur **âœ… Utiliser cette configuration**

### 3. Lancer l'entraÃ®nement

1. Allez dans **ğŸ‹ï¸ EntraÃ®nement PPO**
2. Configurez les paramÃ¨tres :
   - **Timesteps** : 50 000 (pour un test rapide)
   - **Learning rate** : 0.0003
3. Cliquez sur **ğŸš€ Lancer l'entraÃ®nement**

!!! tip "Conseil"
    Pour de meilleurs rÃ©sultats, utilisez au moins 100 000 timesteps.

### 4. Ã‰valuer le modÃ¨le

1. Allez dans **ğŸ“Š Ã‰valuation**
2. SÃ©lectionnez votre modÃ¨le entraÃ®nÃ©
3. Cliquez sur **Ã‰valuer** pour comparer avec les baselines

## Option 2 : Ligne de Commande

### EntraÃ®nement simple

```bash
python scripts/train.py --config rouleurs --timesteps 50000
```

### EntraÃ®nement avec options avancÃ©es

```bash
python scripts/train.py \
    --config rouleurs \
    --timesteps 100000 \
    --learning-rate 0.0003 \
    --n-steps 2048 \
    --batch-size 64 \
    --name mon_premier_modele
```

### Ã‰valuation

```bash
python scripts/evaluate.py --model models/mon_premier_modele/best_model.zip
```

## Structure des RÃ©sultats

AprÃ¨s l'entraÃ®nement, vous trouverez :

```
models/
â””â”€â”€ mon_premier_modele/
    â”œâ”€â”€ best_model.zip      # Meilleur modÃ¨le (sauvegardÃ© par callback)
    â”œâ”€â”€ final_model.zip     # ModÃ¨le final
    â”œâ”€â”€ vec_normalize.pkl   # Normalisation VecNormalize
    â”œâ”€â”€ monitor.csv         # Logs d'entraÃ®nement
    â””â”€â”€ config.json         # Configuration utilisÃ©e
```

## Monitoring avec TensorBoard

Pour suivre l'entraÃ®nement en temps rÃ©el :

```bash
tensorboard --logdir logs/tensorboard
```

Puis ouvrez `http://localhost:6006` dans votre navigateur.

### MÃ©triques Importantes

| MÃ©trique | Description | Bon signe |
|----------|-------------|-----------|
| `rollout/ep_rew_mean` | RÃ©compense moyenne | â†—ï¸ Croissante |
| `rollout/ep_len_mean` | Longueur des Ã©pisodes | Stable |
| `train/loss` | Perte totale | â†˜ï¸ DÃ©croissante |
| `train/entropy_loss` | Entropie | DÃ©croÃ®t lentement |

## Exemple de Code Python

```python
from config import get_example_config
from environments import EnvironmentRegistry
from agents import PPOTrainer

# Charger la configuration
config = get_example_config('rouleurs')

# CrÃ©er l'environnement
env = EnvironmentRegistry.create('strategic', config)

# CrÃ©er et entraÃ®ner l'agent
trainer = PPOTrainer(
    config=config,
    total_timesteps=50000,
    learning_rate=3e-4
)

model = trainer.train()

# Ã‰valuer
results = trainer.evaluate(n_episodes=10)
print(f"RÃ©compense moyenne: {results['mean_reward']:.2f}")
```

## Prochaines Ã‰tapes

- â¡ï¸ [Interface Streamlit](streamlit.md) - Guide complet de l'interface
- â¡ï¸ [Configuration avancÃ©e](../user-guide/configuration.md) - Personnaliser les paramÃ¨tres
- â¡ï¸ [Comprendre le PDP](../concepts/pdp.md) - ThÃ©orie sous-jacente
